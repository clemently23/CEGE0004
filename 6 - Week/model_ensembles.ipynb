{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Ensembles\n",
    "\n",
    "In this notebook you will learn how to implement some ensembles models in scikit-learn.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "In this notebook you will be working with the Breast Cancer Wisconsin\n",
    "[dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "\n",
    "This dataset is a classic and easy binary classification dataset. Its features are computed from a digitized image of a\n",
    "fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "In the original dataset you have 32 attributes:\n",
    "1. An id;\n",
    "1. The diagnosis (M = malignant, B = benign);\n",
    "1. 32 real-valued features computed for each cell nucleus:\n",
    "    1. radius (mean of distances from center to points on the perimeter);\n",
    "    1. texture (standard deviation of gray-scale values);\n",
    "    1. perimeter;\n",
    "    1. area;\n",
    "    1. smoothness (local variation in radius lengths);\n",
    "    1. compactness (perimeter^2 / area - 1.0);\n",
    "    1. concavity (severity of concave portions of the contour);\n",
    "    1. concave points (number of concave portions of the contour);\n",
    "    1. symmetry;\n",
    "    1. fractal dimension (\"coastline approximation\" - 1).\n",
    "\n",
    "The dataset is made of 569 instances and has a dimensionality of 30. Of these instances, 212 belong to the malignant\n",
    "class and 357 to the benign class.\n",
    "\n",
    "To load this dataset we will use the `load_brest_cancer` function of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "xs = dataset.data\n",
    "ys = dataset.target\n",
    "\n",
    "print('The dimensionality of the dataset is', xs.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a quick look at the loaded dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now perform the train-test split. We will use 80% for the training set and 20% for the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since all variables are continues, we perform a min-max scaling of both, the training set and test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(xs_train)\n",
    "\n",
    "xs_train = scaler.transform(xs_train)\n",
    "xs_test = scaler.transform(xs_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stacking\n",
    "\n",
    "The key idea of stacking is to train multiple classifiers and then stack them together using a meta-learner or a voting\n",
    "mechanism. In this case we will use a SVM classifier, a naive Bayes classifier, a decision tree classifier and\n",
    "a neural network, all stacked together using a majority voting mechanism.\n",
    "\n",
    "Let's start with the definition of these 4 classifiers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "svm_clf = SVC(C=0.1)\n",
    "nb_clf = MultinomialNB()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "mlp_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then define the voting mechanism by using the `VotingClassifier` of scikit-learn."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', svm_clf), ('nb', nb_clf), ('df', dt_clf), ('mlp', mlp_clf)],\n",
    "    voting='hard')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use this voting classifier we need to provide an id for each classifier and the classifiers themselves, and a voting\n",
    "parameter which can be set to `hard` or `soft`. If this is set to ‘hard’, it uses the predicted class labels for a\n",
    "majority rule voting, if this is set to ‘soft’, it predicts the class label based on the argmax of the sums of the\n",
    "predicted probabilities.\n",
    "\n",
    "We then fit the voting classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voting_clf.fit(xs_train, ys_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we do not need to forget to fit also each individual classifier independently.\n",
    "We will do this in the following for loop."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for clf in (svm_clf, nb_clf, dt_clf, mlp_clf):\n",
    "    clf.fit(xs_train, ys_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now measure the train and test accuracy of each classifier, including the ensembled one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (svm_clf, nb_clf, dt_clf, mlp_clf, voting_clf):\n",
    "    print(clf.__class__.__name__)\n",
    "    ys_pred = clf.predict(xs_train)\n",
    "    print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
    "    ys_pred = clf.predict(xs_test)\n",
    "    print('\\ttest:', accuracy_score(ys_test, ys_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bagging\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original\n",
    "dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n",
    "\n",
    "In the following example we will perform the bagging of kNN classifiers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters `max_samples` and `max_features` control the fraction of examples and features considered in\n",
    "each replica of the dataset.\n",
    "\n",
    "We now fit this classifier and evaluate its accuracy on the train and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag_clf.fit(xs_train, ys_train)\n",
    "\n",
    "print(bag_clf.__class__.__name__, '(kNN)')\n",
    "ys_pred = bag_clf.predict(xs_train)\n",
    "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
    "ys_pred = bag_clf.predict(xs_test)\n",
    "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest is a famous ensemble based model that fits a number of decision tree classifiers on various sub-samples of\n",
    "the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is\n",
    "controlled with the `max_samples` parameter if `bootstrap=True`, otherwise the whole dataset is used to build each tree."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now fit this classifier and evaluate its accuracy on the train and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_clf.fit(xs_train, ys_train)\n",
    "\n",
    "print(rf_clf.__class__.__name__)\n",
    "ys_pred = rf_clf.predict(xs_train)\n",
    "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
    "ys_pred = rf_clf.predict(xs_test)\n",
    "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Boosting (AdaBoost)\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then\n",
    "fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances\n",
    "are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "For this classifier we will boost 200 times decision trees of depth 1, aka decision stumps."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameter `n_estimators` controls the maximum number of estimators at which boosting is terminated.\n",
    "Of course, in case of a perfect fit, the learning procedure is stopped earlier.\n",
    "\n",
    "We now fit this classifier and evaluate its accuracy on the train and test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ada_clf.fit(xs_train, ys_train)\n",
    "\n",
    "print(ada_clf.__class__.__name__, '(DecisionStumps)')\n",
    "ys_pred = ada_clf.predict(xs_train)\n",
    "print('\\ttrain:', accuracy_score(ys_train, ys_pred))\n",
    "ys_pred = ada_clf.predict(xs_test)\n",
    "print('\\ttest:', accuracy_score(ys_test, ys_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in all the examples above we have not performed any validation.\n",
    "How would you perform the validation of these classifiers?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}